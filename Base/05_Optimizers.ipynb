{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고속 옵티마이저\n",
    "\n",
    "표준적인 경사하강법 대신 더 빠른 옵티마이저를 사용하는 것은 굉장히 좋은 선택입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import io\n",
    "with ZipFile(\"../Data/고객 대출등급 분류 해커톤.zip\",\"r\") as f:\n",
    "    data=io.BytesIO(f.read(\"고객 대출등급 분류 해커톤/train.csv\"))\n",
    "from sklearn import model_selection as mod\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data=pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준적인 경사하강법은 다음과 같은 방법임을 우리는 알고있습니다.\n",
    "\n",
    "$\\theta$ -> $\\theta - \\eta \\triangledown_{\\theta} J(\\theta)$\n",
    "\n",
    "여기서 $J(\\theta)$ 는 $\\theta$에 따른 손실(자코비안)행렬(gradient)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀(momentum)은 관성이라는 관점에서 이 경사도를 이해합니다.\n",
    "\n",
    "$m$ -> $\\beta m- \\eta \\triangledown_{\\theta} J(\\theta)$\n",
    "\n",
    "$\\theta$ -> $\\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 이전 그레디언트의 일부를 관성으로 이해하고 있습니다.(일반적으로 $\\beta$=0.9 입니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네스테로프 가속 경사\n",
    "\n",
    "위의 모멘텀 초기화에 한가지 가정이 더해집니다.(일반적으로 맞습니다)\n",
    "\n",
    "- 모멘텀은 언제나 최적점을 향하는 방향을 가르킵니다.\n",
    "\n",
    "따라서 위의 식을 다음과 같이 변경합니다.\n",
    "\n",
    "$m$ -> $\\beta m- \\eta \\triangledown_{\\theta} J(\\theta+\\beta m)$\n",
    "\n",
    "$\\theta$ -> $\\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉 경사(gradient)를 모멘텀만큼 이동시킨 뒤에 측정한다는 발상입니다.\n",
    "\n",
    "여기까지 이해했다면 아래의 파라미터들이 눈에 익을 겁니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 16:22:41.429091: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-01-13 16:22:41.429109: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-01-13 16:22:41.429115: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2025-01-13 16:22:41.429134: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-13 16:22:41.429153: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.sgd.SGD at 0x16ce858b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(learning_rate=0.01,momentum=0.9,nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 생각하면 가장 가파른 방향을 찾아 변화를 줍니다.\n",
    "\n",
    "문제는 각 변수마다의 변화량 차이가 클 경우 발생합니다.\n",
    "\n",
    "AdaGrad는 이런 경우를 표준화를 통해 완화시키고자 하는 아이디어입니다.\n",
    "\n",
    "아래와 같습니다.\n",
    "\n",
    "$s \\rightarrow s+ \\triangledown J(\\theta) \\circ \\triangledown J(\\theta)$ ($\\circ$ 은 원소별 곱셈을 나타냅니다)\n",
    "\n",
    "$\\theta \\rightarrow \\theta - \\eta \\triangledown J \\oslash \\sqrt{s+\\epsilon}$ ($\\oslash$는 원소별 나눗셈을 나타냅니다)\n",
    "\n",
    "이를 각 $\\theta_i \\in \\theta$ 관점에서 보면 $\\eta$의 값이 감소하는 것과 같습니다.(두번쨰 단계에서)\n",
    "\n",
    "따라서 이를 __적응형 학습률__ 이라 부릅니다.(즉 학습률 튜닝에 대한 중요도가 타 옵티마이저보다 떨어집니다)\n",
    "\n",
    "다만 단점은 이차방정식 문제에서는 잘 작동하지만, 그외에 지나치게 일찍 멈춰 성능을 보장하기 어렵다는 것입니다.\n",
    "\n",
    "(잘 사용하지 않지만 이후 옵티마이저의 기본이 되기에 설명했습니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "지나치게 빠르게 수렵하는 AdaGrad 의 문제를 해결하기 위해 s에 감쇠율을 적용합니다.\n",
    "\n",
    "> <참고>\n",
    "\n",
    "    지나치게 빠르게 수렵하는 AdaGrad의 문제는 그레디언트의 크기에 의해 발생합니다\n",
    "\n",
    "    그레이언트가 큰 가파른 지역에서 $s$는 커집니다. 따라서 학습률을 낮추는 역할을 합니다. 이로 인해 최적점 도착전에 수렴하는 문제가 발생합니다.\n",
    "\n",
    "위의 식을 보면 $s$는 모든 그레디언트의 원소곱을 누적하고 있음을 알 수 있습니다.\n",
    "\n",
    "이를 비틀어 RMSProp는 최근 $s$(최근 그레디언트)에 가중치를 줍니다.\n",
    "\n",
    "$s \\rightarrow \\rho s + (1-\\rho) \\triangledown J(\\theta) \\circ \\triangledown J(\\theta)$ ($\\rho$ 는 `rho`로 읽습니다)\n",
    "\n",
    "이는 초기 $s$는 $\\rho$에 의해 지수감소하게 만듭니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.rmsprop.RMSprop at 0x31f4460a0>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9) #파라미터 rho 가 왜 있는지 이해가 되죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다행히 기본값으로도 잘 작동합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Adam 은 `adaptive momentum esitmation`(적응형 모멘텀 추정)을 뜻합니다.\n",
    "\n",
    "즉 위의 AdaGrad 와 RMSProp 의 아이디어를 합칩니다.\n",
    "\n",
    "$t$는 반복횟수입니다.\n",
    "\n",
    "$m \\rightarrow \\beta_{1}m -(1-\\beta_{1})\\triangledown J_{\\theta}$\n",
    "\n",
    "$\\hat{m} \\leftarrow \\frac{m}{1-\\beta_{1}^{t}}$\n",
    "\n",
    "$s \\rightarrow \\beta_{2}s + (1-\\beta_2) \\triangledown J_{\\theta} \\circ \\triangledown J_{\\theta}$\n",
    "\n",
    "$\\hat{s} \\leftarrow \\frac{s}{1-\\beta_2^t}$\n",
    "\n",
    "$\\theta \\leftarrow \\theta + \\eta \\hat{m} \\oslash \\sqrt{\\hat{s}+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad 의 장점을 가져오면서 당연히 __적응형 학습률__ 의 특징을 잘 계승합니다.\n",
    "\n",
    "즉 학습률 튜닝의 중요성을 낮춰주는 좋은 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.adam.Adam at 0x31f3b7d00>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\n",
    "Adam 의 방법에 Nesterov 기법을 더한 것입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW\n",
    "\n",
    "SGD 를 사용할 떄 $l_2$ 규제를 사용하는 것은 가중치의 크기를 줄이는 좋은 규제방법입니다.\n",
    "\n",
    "이와 비슷한 방법으로 `가중치 감쇠`(모델의 가중치에 0.99와 같은 가중치를 감쇠계수를 곱하는 방법)이 있습니다.\n",
    "\n",
    "실제 SGD를 사용할 떄 위의 가중치 감쇠 방법과 $l_2$규제가 동일한 결과를 냄이 밝혀졌지만, \n",
    "\n",
    "Adam과 그의 변형방법들 (AdamMax,Nadam 등)은 그렇지 않습니다.\n",
    "\n",
    "오히려 SGD 가 만든 일반적인 모델을 만들지 못했습니다($l_2$ 규제를 적용한 결과)\n",
    "\n",
    "따라서 가중치 감쇠방법을 적절히 사용해 위의 문제를 해결한 방법이 AdamW 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.adamw.AdamW at 0x31f4fd0a0>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999)\n",
    "\n",
    "# weight_decay가 가중치 감쇠입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
