{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training\n",
    "\n",
    "모델의 훈련에는 다양한 요소가 들어있습니다. 이전에 알아둔 모델의 생성 이후 우리는 어떤 고민을 해야하고 어떤 해결방법을 택할 수 있는지 알아보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainable\n",
    "\n",
    "훈련 가능하다는 것은 뭐일까? \n",
    "\n",
    "아래의 예시를 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import io\n",
    "with ZipFile(\"../Data/고객 대출등급 분류 해커톤.zip\",\"r\") as f:\n",
    "    data=io.BytesIO(f.read(\"고객 대출등급 분류 해커톤/train.csv\"))\n",
    "\n",
    "from pandas import read_csv\n",
    "data=read_csv(data)\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대출금액</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       대출금액\n",
       "0  12480000\n",
       "1  14400000\n",
       "2  12000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"대출금액\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 예시에서 우리는 위의 특성에 대해 `Standard Scale`을 적용했습니다.(즉 정규화)\n",
    "\n",
    "이를 다음과 같이 진행할 수도 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "variance=tf.constant(data[[\"대출금액\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 층 설정\n",
    "standard_layers=keras.layers.Normalization()\n",
    "standard_layers.adapt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층 예시\n",
    "hidden_layer=keras.layers.Dense(1,activation=\"relu\")\n",
    "hidden_layer.build(input_shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 정규화층과 은닉층의 학습가능한 변수를 확인해보죠\n",
    "\n",
    "아래와 같이 정규화 층은 어떠한 연산곱을 실행하지 않기때문에(다만 `adapt`메소드를 통해 분산과 평균값을 가지고 있습니다)\n",
    "\n",
    "은닉층(Dense)층과 다르게 훈련가능한 변수가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_layers.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=dense_6/kernel, shape=(1, 1), dtype=float32, value=[[1.303249]]>,\n",
       " <Variable path=dense_6/bias, shape=(1,), dtype=float32, value=[0.]>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-0.56384814],\n",
       "       [-0.37796414],\n",
       "       [-0.61031914],\n",
       "       [-0.37796414],\n",
       "       [-0.02943163]], dtype=float32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_layers(variance)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer(standard_layers(variance)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 요점은 trainable variance 가 훈련 중 수정될 주요 변수라는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 방법\n",
    "\n",
    "모델의 훈련 방법은 자동후진미분을 통해 이루어집니다.\n",
    "\n",
    "또한 전통적인 방법으로 경사하강법을 적용하는데, 이에 대해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss(손실률)\n",
    "\n",
    "역전파를 통한 파라미터를 수정하는 과정에서 우선적으로 `loss`(손실량)를 결정해야합니다.\n",
    "\n",
    "여기서 당연하겠지만, 우리는 손실량을 __최소__ 로 하는게 목적입니다.\n",
    "\n",
    "예를 들어 loss를 `accuracy`로 결정하는 것은 목적에 부합하지않고, error rate=1-accuracy 로 손실을 설정하는게 \n",
    "\n",
    "목적에 부합한 loss 설정방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow(Keras)에서는 Loss 를 살펴보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.losses.losses.BinaryCrossentropy"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이진분류의 엔트로피\n",
    "keras.losses.BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM 의 최대 마진 손실함수\n",
    "keras.losses.Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 영향을 완화하기 위한 손실함수\n",
    "keras.losses.Huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 우리가 자주 접하는 MSE, MAE 등 다양한 손실함수를 지원하므로 한번씩 살펴보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "\n",
    "위의 손실함수가 결정되었다면 우리는 역전파를 통해 각 노드의 gradient 를 계산할 수 있습니다.\n",
    "\n",
    "이는 `Tensorflow.GradientTape`를 활용하면 쉽게 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 모델을 만들어 살펴보죠.\n",
    "\n",
    "1. 연속형 특성을 사용하죠.\n",
    "2. 은닉층은 2층만 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "use_feature=data.select_dtypes(np.number)\n",
    "use_label=data.대출등급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=data.대출등급.unique()\n",
    "label_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자로 변환\n",
    "use_label=use_label.map(lambda x:np.argmax(label_list==x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.24800e+07, 7.20000e+07, 1.89000e+01, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [1.44000e+07, 1.30800e+08, 2.23300e+01, ..., 2.34060e+05,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [1.20000e+07, 9.60000e+07, 8.60000e+00, ..., 1.51944e+05,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       ...,\n",
       "       [1.44000e+07, 8.40000e+07, 1.12400e+01, ..., 2.41236e+05,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [1.56000e+07, 6.63300e+07, 1.73000e+01, ..., 8.18076e+05,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [8.64000e+06, 5.04000e+07, 1.18000e+01, ..., 2.74956e+05,\n",
       "        0.00000e+00, 0.00000e+00]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_layers=keras.layers.Normalization()\n",
    "standard_layers.adapt(use_feature.values)\n",
    "\n",
    "Simple_model=keras.Sequential(\n",
    " [keras.layers.Input(shape=use_feature.shape[-1:]),\n",
    "  standard_layers,\n",
    "  keras.layers.Dense(9,activation=\"relu\"),\n",
    "  keras.layers.Dense(9,activation=\"relu\"),\n",
    "  keras.layers.Dense(7,activation=\"sigmoid\")]   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 간단한 모델에서 훈련 가능한 변수를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=sequential_4/dense_21/kernel, shape=(9, 7), dtype=float32, value=[[-0.11383507 -0.00220287  0.06821764  0.5739718  -0.5091083   0.35995752\n",
       "   -0.30687857]\n",
       "  [ 0.4870692   0.2801087   0.06435692  0.458798    0.11078292  0.36766064\n",
       "    0.03970134]\n",
       "  [ 0.3055752  -0.51906633 -0.49382025 -0.41563368  0.26066095 -0.19154072\n",
       "   -0.50225806]\n",
       "  [ 0.3222494   0.5239642  -0.4780228  -0.5641748   0.56159717  0.03943264\n",
       "   -0.4706429 ]\n",
       "  [ 0.22212398 -0.17264467  0.19317448  0.5103157   0.32983398  0.40341145\n",
       "   -0.2512633 ]\n",
       "  [-0.44533098 -0.27005273 -0.5697107   0.52962214  0.43648583  0.4109463\n",
       "    0.25538373]\n",
       "  [-0.17670673  0.50366515  0.28261262 -0.19724512  0.28348935  0.15907198\n",
       "   -0.59133196]\n",
       "  [ 0.3777575  -0.16513804 -0.29112315  0.52439624 -0.33534116  0.15115249\n",
       "    0.57314974]\n",
       "  [ 0.43179315 -0.15831149 -0.41772968 -0.08764207  0.12076485 -0.53711\n",
       "   -0.19404975]]>,\n",
       " <Variable path=sequential_4/dense_21/bias, shape=(7,), dtype=float32, value=[0. 0. 0. 0. 0. 0. 0.]>]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막 output layer 층\n",
    "Simple_model.trainable_weights[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss 선정 및 간단한 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_loss=keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 첫번째 데이터의 예측값을 나타냅니다.\n",
    "(각 라벨당의 확률을 나타냄과 같은 효과입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.4827957  0.3738754  0.38036695 0.7468218  0.70034134 0.6690052\n",
      "  0.47923845]], shape=(1, 7), dtype=float32)\n",
      "예측된 y: D\n",
      "실제의 y: C\n"
     ]
    }
   ],
   "source": [
    "pred_y=Simple_model(use_feature.values[:1,:])\n",
    "print(pred_y)\n",
    "\n",
    "print(\"예측된 y:\",label_list[tf.argmax(pred_y[0])])\n",
    "print(\"실제의 y:\",label_list[use_label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.3101218>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 손실값은 다음과 같이 계산됩니다.\n",
    "\n",
    "use_loss(y_true=use_label[0],y_pred=pred_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient 를 계산해보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    pred_y=Simple_model(use_feature.values[:1,:])\n",
    "    loss=use_loss(use_label[0],pred_y[0])\n",
    "grad=tape.gradient(loss,Simple_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 층(노드)에 해당하는 손실함수에 대한 gradient 는 다음과 같습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(9, 7), dtype=float32, numpy=\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.07311683,  0.06854559, -0.62633556,  0.05536497,  0.06145093,\n",
       "          0.06483992,  0.07307728],\n",
       "        [ 0.06593559,  0.06181332, -0.5648194 ,  0.04992725,  0.05541547,\n",
       "          0.05847161,  0.06589993],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.02000165,  0.01875116, -0.17133872,  0.0151455 ,  0.01681036,\n",
       "          0.01773744,  0.01999083]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
       " array([ 0.06515528,  0.06108179, -0.55813503,  0.04933638,  0.05475965,\n",
       "         0.05777963,  0.06512003], dtype=float32)>]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "드디어 위의 계산된 gradient를 적용할 차례입니다.\n",
    "\n",
    "이를 `Tensorflow(keras)`에서 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전통적인 경사하강법\n",
    "sgd=keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Variable path=SGD/iteration, shape=(), dtype=int64, value=1>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.apply_gradients(zip(grad,Simple_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 층의 `bias`가 어떻게 갱신됐는지 보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=sequential_4/dense_21/bias, shape=(7,), dtype=float32, value=[-0.00065155 -0.00061082  0.00558135 -0.00049336 -0.0005476  -0.0005778\n",
       "  -0.0006512 ]>]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simple_model.trainable_variables[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.48339954 0.37522963 0.38878614 0.7444099  0.6976011  0.66783875\n",
      "  0.47791767]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred_y=Simple_model(use_feature.values[:1,:])\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 네트워크 훈련의 아주 작은 스텝이 어떻게 이루어짐을 알아보았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
