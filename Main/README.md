## 요약

<핸즈온 머신러닝> 을 기본적으로 하며, 이해가 쉽게 순서와 난이도를 조절해놨습니다.

## 목차

0. KerasModel
    
    필요에 따라 모델 생성방법을 결정하기 위해
    keras 를 활용한 모델의 생성방법 3가지를 설명합니다.

    - 1. Sequential API
    - 2. Functional API
    - 3. SubClassing API

1. TrainableVariance

    Neural Network 릐 기본적인 학습방법에 대한 이해를 돕기 위해 구성했습니다.
    
    해당 노트에서 Tensorflow를 이용한 역전파가 어떻게 적용되고있는지, 최대한 눈으로 확인하게끔 코드를 구성했습니다.

    <이론적인 설명은 경사하강법을 설명한 노트를 참고>

2. ModelTrain

    모델을 만드는 방법, 모델이 훈련하는 방법에 대한 이해를 기반으로,

    `keras`를 통한 모델의 훈련 순서

    - 1. compile  :  작성된 모델이 실행되는 환경을 세팅합니다

    - 2. fit :  위의 환경 하에서 학습을 시작합니다.

    각 순서에 필요한 주요 파라미터와 해당 파라미터는 어디에 귀속되어있는지에 대한 설명입니다.

3. Gradien Problem

    모델을 훈련시킬 때 벌생할 수 있는 그레디언트 손실, 발산 문제에 대한 설명입니다.

    이 문제에 대한 초기화 전략과 다양한 활성화 함수, BN 기법에 대한 설명입니다.

    - 1. 초기화
        * 르쿤 초기화, 글로럿 초기화, He 초기화
        * 사용자 정의 초기화 설정

    - 2. 활성화 함수
        * LeakyRELU, ELU (부드럽지 않은 RELU 변형)
        * SELU (특정조건하에 굉장히 강력한 활성화 함수)
        * Mish, Swish

    - 3. BN(배치정규화 기법)
        > 각 층에 입력(혹은 출력값)을 원점을 기준으로 초기화하는 전략입니다.
    - 4. 그레디언트 클리핑
        > 그레디언트 기준값을 설정하는 방법

4. Training Speed

    모델 학습시 학습속도 향상에 도움을 주는 다양한 방법에 대한 설명입니다.

    - 1. Tensorflow Data API (다음장에서 설명할 예정)
    - 2. 전이학습
        > 비슷한 조건의 네트워크 모델의 일부를 재사용하는 방법
    - 3. 고속옵티마이저 (다다음 장에서 설명할 예정)
    - 4. 학습률 스케줄링(강조된 스케줄링이 선호되는 방법들입니다)
        * 거듭제곱 
        * __지수__
        * 구간별 고정
        * 성능 기반
        * __1사이클__

5. Optimizers

    표준적인 경사하강법을 벗어나 더 빠르게 최적점을 찾기 위한 여러가지 optimizers 를 소개합니다.
    (강조된 optmizer 가 선호되고 있습니다)

    - 1. 모멘텀 최적화
    - 2. __Nesterov accelerated gradient__ :  모멘텀 최적화 방법의 변형
    - 3. AdaGrad :  적응형 학습률을 만드는 방법
    - 4. __Adam__ :  모멘텀 최적화와 AdaGrad 의 결합
    - 5. __Nadam__ :  Nesterov 와 AdaGrad 의 결합
    - 5. __AdamW__ :  가중치감쇠 방법을 통한 AdaGrad의 일반화 성능 강화


        

    
